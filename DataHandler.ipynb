{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "# Node and Network\n",
    "from Node import Node\n",
    "from Network import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, cluster: Cluster, data: jnp.ndarray = jnp.array([]), target: jnp.ndarray = jnp.array([]), encoding : str = \"spike_amp\"): # TODO, encoding type, decoding type, start iterations, number of iterations to get the network started, batch size, sampling steps\n",
    "        \"\"\"\n",
    "        This class handles input and output data.\n",
    "        For now it only offer a simple way to input and output data trough a network. \n",
    "\n",
    "        Data encoding uses algorithms similar to SNNs\n",
    "\n",
    "        # NOTE: Optimization is handled by the Cluster class, maybe change to runner class\n",
    "        # TODO: Diffrerent learning type depending on network running type, pretraining, running...\n",
    "        # TODO: Create subclasses which read in diffrent data formats, like csv, images, ...\n",
    "        # TODO: Batch size and sampling steps\n",
    "        # TODO: Dependency on network type, like pretrained or functional\n",
    "        # TODO: Put some features in sperate network runner\n",
    "        # TODO: Error handling\n",
    "Planned:\n",
    "- Encoding types (rate dependent and continous spike dependent)\n",
    "- Output decoding (takes the average, median, ...)\n",
    "- Start iterations, number of iterations to get the network started\n",
    "\n",
    "        Args:\n",
    "            cluster (Cluster): Cluster to which the data is fed\n",
    "            data (jnp.ndarray): Input data\n",
    "            target (jnp.ndarray): Target data\n",
    "        \"\"\"\n",
    "        # size definitions from the cluster\n",
    "        self.cluster = cluster\n",
    "        self.input_size = len(cluster.input_nodes)\n",
    "        self.output_size = len(cluster.output_nodes)\n",
    "        # data\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.encoding = encoding\n",
    "        # size check\n",
    "        # TODO : Make same error type as used in other files\n",
    "        assert len(data) == len(target), \"Data and target size mismatch\"\n",
    "        assert len(data[0]) == self.input_size, \"Data input size mismatch\"\n",
    "        assert len(target[0]) == self.output_size, \"Data output size mismatch\"\n",
    "        # data info\n",
    "        self.index = 0\n",
    "        self.data_size = len(data)\n",
    "\n",
    "    # TODO: Create subclasses, which take images, csv, ...\n",
    "    def load_data(self, data):\n",
    "        \"\"\"\n",
    "        Takes in Data and stores it in the class to be later accessible by the runner\n",
    "        # TODO: Implement functions to load data from diffrent formats\n",
    "\n",
    "        Args:\n",
    "            data (jnp.ndarray): Data to be loaded\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implemented trough a subclass\")\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO : Check working\n",
    "    Idea for Csv read in\n",
    "    @classmethod\n",
    "    def from_csv(cls, path):\n",
    "        # convert csv to jnp array\n",
    "        data = jnp.loadtxt(csv_file_path, delimiter=',')\n",
    "        target = jnp.zeros(data.shape)\n",
    "        return cls(cluster, data, target, encoding_type, decoding_type, batch_size, sampling_steps) # call cunstructor\n",
    "    \"\"\"\n",
    "    # TODO: other formats and images\n",
    "    \n",
    "# Signal processing\n",
    "    def encode_spike_amp(self, cur_data):\n",
    "        \"\"\"\n",
    "        Encodes the data into a spike amplitude format.\n",
    "        \n",
    "        This format creates an array of the diffrent signal timesteps used for running the network\n",
    "        \"\"\"\n",
    "        # give out the current data\n",
    "        spike_amp_data = jnp.array([cur_data]) # TODO: Check if necessary\n",
    "        # create zeros for the next spike\n",
    "        zeros = jnp.zeros(spike_amp_data.shape)\n",
    "        # merge to for two amplitudes for a spike\n",
    "        cur_data = jnp.vstack([cur_data, zeros])\n",
    "        return cur_data\n",
    "    \n",
    "    # TODO: Implement rate encoding\n",
    "\n",
    "# Iterator\n",
    "    \"\"\"\n",
    "    The iterator includes has batch size elements\n",
    "    The elements have 2 features one is temporal and the other is the data for each input\n",
    "    The temporal features should be switched in a circle for each iteration of the network\n",
    "    \"\"\"\n",
    "    def  __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        # TODO get batch\n",
    "        # Return one element for now\n",
    "\n",
    "        # select encoding mode and create elements\n",
    "        if self.encoding == \"spike_amp\":\n",
    "            cur_data = self.encode_spike_amp(self.data[self.index])\n",
    "            cur_target = self.target[self.index]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Encoding not implemented, try spike_amp\")\n",
    "        # increment index\n",
    "        self.index += 1\n",
    "        # check if index is out of bounds\n",
    "        if self.index == self.data_size + 1: # TODO: One bigger sice data is accesed before\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        return cur_data, cur_target\n",
    "\n",
    "# Helper functions\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the index of the data to 0\n",
    "        \"\"\"\n",
    "        self.index = 0\n",
    "    \n",
    "    def convert_data(self, data, encoder):\n",
    "        \"\"\" \n",
    "        Converts the data into the encoding format\n",
    "\n",
    "        Args:\n",
    "            data (jnp.ndarray): Data to be converted\n",
    "            encoder (str): Encoding type\n",
    "        \"\"\"\n",
    "        if encoder == \"spike_amp\":\n",
    "            return self.encode_spike_amp(data)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Encoding not implemented, try spike_amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4. 5.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1., 2., 3., 4., 5.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming cur_data is already a jnp.array\n",
    "cur_data = jnp.array([1, 2, 3, 4, 5])  # Example data\n",
    "spike_amp_data = jnp.array([cur_data])\n",
    "\n",
    "spike_amp_data = jnp.vstack([spike_amp_data, jnp.zeros(cur_data.shape)])  # Append a zero array to the spike_amp_data\n",
    "\n",
    "# Print the spike_amp_data to see its contents\n",
    "print(spike_amp_data)\n",
    "# If using a Jupyter Notebook, you can also display it\n",
    "spike_amp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and target size (3, 1) (3, 1)\n",
      "Data and target first element [1] [1]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Data input size mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData and target first element\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;241m0\u001b[39m], target[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     23\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m DataHandler(cluster, data, target)\n\u001b[1;32m---> 24\u001b[0m data_handler_big \u001b[38;5;241m=\u001b[39m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_big\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_big\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, cluster, data, target, encoding)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# size check\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# TODO : Make same error type as used in other files\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(target), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData and target size mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData input size mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData output size mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# data info\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Data input size mismatch"
     ]
    }
   ],
   "source": [
    "# Test DataHandler\n",
    "# Nodes\n",
    "# Create a few more nodes for testing\n",
    "Node0 = Node(0, jnp.array([]), jnp.array([]), 0)\n",
    "Node1 = Node(0, jnp.array([]), jnp.array([]), 1)\n",
    "Node2 = Node(0, jnp.array([]), jnp.array([]), 2)\n",
    "nodes = [Node0, Node1, Node2]\n",
    "# Create a cluster, which is just a short linear connection\n",
    "cluster = Cluster(1, 1, nodes=nodes, init_net=False)\n",
    "cluster_big = Cluster(3, 3, nodes=nodes, init_net=False)\n",
    "# add connections\n",
    "cluster.add_connection(Node0, Node1) # NOTE: This needs to be changed when changed in cluster\n",
    "cluster.add_connection(Node1, Node2)\n",
    "# TODO: length finding for priming\n",
    "# DataLoader\n",
    "# Frist dimension is the element, second is the data\n",
    "data = jnp.array([[1],[2],[3]]) # jnp.array([[1, 2, 3],[1,4,6]]) for 3 inputs\n",
    "target = jnp.array([[1],[2],[3]])\n",
    "data_big = jnp.array([[1, 2, 3],[1,4,6]])\n",
    "target_big = jnp.array([[1, 2, 3],[1,4,6]])\n",
    "print(\"Data and target size\", data.shape, data.shape)\n",
    "print(\"Data and target first element\", data[0], target[0])\n",
    "\n",
    "data_handler = DataHandler(cluster, data, target)\n",
    "data_handler_big = DataHandler(cluster, data_big, target_big)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(data_handler.convert_data(data[1], \"spike_amp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]]\n",
      "[1]\n",
      "(1,)\n",
      "inputs:  [Array([0.], dtype=float32), Array([0.], dtype=float32), Array([], shape=(0,), dtype=float32)]\n",
      "out_conn:  [[1], [2], []]\n",
      "in_conn:  [[], [0], [1]]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "cluster.print_connections()\n",
    "cluster.get_neighbors(Node0, 1)\n",
    "input = jnp.array([1])\n",
    "print(input)\n",
    "print(input.shape)\n",
    "print(cluster.run(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[0.]\n",
      "[2.]\n",
      "[0.]\n",
      "[3.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "for data, target in data_handler:\n",
    "    for part in data:\n",
    "        print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in data_handler_big:\n",
    "    for part in data:\n",
    "        print(part)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_expanding_NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
